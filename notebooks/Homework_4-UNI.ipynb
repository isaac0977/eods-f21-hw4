{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "### Due: Sat Dec 11th @ 11:59pm ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Recommendations and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will perform two common NLP tasks: \n",
    " 1. Generate recommendations for products based on product descriptions using an LDA topic model.\n",
    " 2. Perform sentiment analysis based on product reviews using sklearn Pipelines.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Follow the comments below and fill in the blanks (____) to complete.\n",
    "- Please **'Restart and Run All'** prior to submission.\n",
    "- **Save pdf in Landscape** and **check that all of your code is shown** in the submission.\n",
    "- When submitting in Gradescope, be sure to **select which page corresponds to which question.**\n",
    "\n",
    "Out of 45 points total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generate Recommendations from LDA Transformation\n",
    "\n",
    "In this part we will transform a set of product descriptions using TfIdf and LDA topic modeling to generate product recommendations based on similarity in LDA space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and transform text using TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (1pts) Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product descriptions \n",
    "#   from the JCPenney department store.\n",
    "\n",
    "# Load product information from ../data/jcpenney-products_subset.csv.zip\n",
    "# Use pandas read_csv function with the default parameters.\n",
    "# Note that this is a compressed version of a csv file (has a .zip suffix).\n",
    "# .read_csv() has a parameter 'compression' with default \n",
    "#     value 'infer' that will handle unzipping the data for us.\n",
    "# Store the resulting dataframe as df_jcp.\n",
    "____\n",
    "\n",
    "# print a summary of df_jcp using .info()\n",
    "# there should be 5000 rows with 2 columns\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. (2pts) Print an Example\n",
    "\n",
    "# The two columns of the dataframe we're interested in are:\n",
    "#   'name_title' which is the name of the product stored as a string\n",
    "#   'description' which is a description of the product stored as a string\n",
    "#\n",
    "# We'll print out the product in the first row as an example\n",
    "# If we try to print both at the same time, pandas will truncate the strings\n",
    "#   so we'll print them seperately\n",
    "\n",
    "# print the name_title column in row 0 of df_jcp\n",
    "____\n",
    "\n",
    "# print the desciption column in row 0 of df_jcp\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. (4pts) Transform Descriptions using TfIdf\n",
    "\n",
    "# In order to pass our product descriptions to the LDA model, we first\n",
    "#   need to vectorize from strings to fixed vectors of floats.\n",
    "# To do this we will transform our documents using TfIdf\n",
    "\n",
    "# Import TfidfVectorizer from sklearn\n",
    "____\n",
    "\n",
    "#  Instantiate a TfidfVectorizer that will\n",
    "#    use both unigrams + bigrams\n",
    "#    exclude terms which appear in less than 5 documents\n",
    "#    exclude terms which appear in more than 20% of the documents\n",
    "#    all other parameters leave as default\n",
    "# Store as tfidf\n",
    "____\n",
    "\n",
    "# fit_transform() tfidf on the descriptions column of df_jcp, \n",
    "#    creating the transformed dataset X_tfidf\n",
    "# Store as X_tfidf\n",
    "____\n",
    "\n",
    "# Print the shape of X_tfidf (should be 5000 x 12347)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: (1pts) Show The Terms Extracted From Row 0\n",
    "\n",
    "# X_tfidf is a matrix of floats, one row per document, one column per vocab term\n",
    "# We can see what terms were extracted, and kept, for the document at df_jcp row 0\n",
    "#   using the .inverse_transform() function\n",
    "# Print the result of calling:\n",
    "#   the .inverse_transform() function of tfidf on the first row of X_tfidf\n",
    "# You should see an array starting with 'show detail'\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. (3pts) Format Bigrams and Print Sample of Extracted Vocabulary \n",
    "\n",
    "# The learned vocabulary can be retrieved from tfidf as a list using .get_feature_names()\n",
    "# Store the extracted vocabulary as vocab\n",
    "____\n",
    "\n",
    "# Sklearn joins bigrams with a space character.\n",
    "# To make our output easier to read, replace the spaces in each term in \n",
    "#    vocab (a list of strings) with an underscore.\n",
    "# To do this we can use the string .replace() method.\n",
    "# For example x.replace(' ','_') will replace all ' ' in x with '_'.\n",
    "# Store the result back into vocab\n",
    "____\n",
    "\n",
    "# Print the last 5 terms in the vocab\n",
    "# The first term printed should be 'zirconia_back'\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform product descriptions into topics and print sample terms from topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (3pts) Perform Topic Modeling with LDA\n",
    "\n",
    "# Now that we have our vectorized data, we can use Latent Direchlet Allocation to learn \n",
    "#   per-document topic distributions and per-topic term distributions.\n",
    "# Though the documents are likely composed of more, we'll model our dataset using \n",
    "#     20 topics for ease of printing.\n",
    "\n",
    "# Import LatentDirichletAllocation from sklearn\n",
    "____\n",
    "\n",
    "# Instantiate a LatentDirichletAllocation model that will\n",
    "#    produce 20 topics\n",
    "#    use all available cores\n",
    "#    random_state=123\n",
    "# Store as lda\n",
    "____\n",
    "\n",
    "# Run fit_transform on lda using X_tfidf.\n",
    "# Store the output (the per-document topic distributions) as X_lda\n",
    "____\n",
    "\n",
    "# Print the shape of the X_lda (should be 5000 x 20)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. (4pts) Get Assigned Topics for Product at df_jcp row 0\n",
    "\n",
    "# Get the assigned topic proportions for the document at row 0 of X_lda\n",
    "# This will be a list of 20 floats between 0 and 1\n",
    "# Round all values to a precision of 2 using\n",
    "# Store as theta_0\n",
    "____\n",
    "print(f'{theta_0 = :}\\n')\n",
    "\n",
    "# LDA will assign a small weight (or proability) to each topic for a document\n",
    "# How many of the topics in theta_0 have a (relatively) large weight (> .01)?\n",
    "# Store in n_assigned_0\n",
    "____\n",
    "print(f'{n_assigned_0 = :}\\n')\n",
    "\n",
    "# What are the indices of the assigned topics, sorted descending by the values in theta_0?\n",
    "# Use np.argsort() to return the indices sorted by value (ascending)\n",
    "# Use [::-1] to reverse the sorting order (from ascending to descending)\n",
    "# Return only the first n_assigned_0 indices, those with large probability\n",
    "# Store as assigned_topics_0\n",
    "# HINT: You should see around 5 indices\n",
    "____\n",
    "print(f'{assigned_topics_0 = :}')\n",
    "\n",
    "# Now that we have the topic indexes, we need to see what each topic looks like\n",
    "#   using the per topic word distrutions stored in lda.components_ (next question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. (5pts) Print Top Topic Terms\n",
    "\n",
    "# To get a sense of what each topic is composed of, we can print the most likely terms for each topic.\n",
    "# We'd like a print statement that looks something like this:\n",
    "#     Topic # 0 : socks spandex fits shoe fits_shoe\n",
    "\n",
    "# To make indexing easier, first convert vocab from a list to np.array()\n",
    "# Store back into vocab\n",
    "____\n",
    "\n",
    "# For each topic print f'Topic #{topic_idx:2d} : ' followed by the top 5 most likely terms in that topic.\n",
    "# Hints: \n",
    "#   The per topic term distributions are stored in lda.components_ \n",
    "#      which should be a numpy array with shape (20, 12347)\n",
    "#   Iterate through the rows of lda.components_, one row per topic\n",
    "#   Use np.argsort() to get the indices of the current row of lda.components_\n",
    "#      sorted by the values in that row in ascending order\n",
    "#   Use [::-1] to reverse the order of the sorted indices\n",
    "#   Use numpy array indexing to get the first 5 index values\n",
    "#   Use these indices to get the corresponding terms from vocab\n",
    "#   Join the list of terms with spaces using ' '.join()\n",
    "#   Each print statement should start with f'Topic #{topic_idx:2d} : ' \n",
    "#      where topic_idx is an integer 0 to 19\n",
    "# The first line should look something like:\n",
    "# Topic # 0 : socks spandex fits shoe fits_shoe\n",
    "\n",
    "# Use as many lines of code as you need\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the description column of row 0, the assigned_topics_0 and \n",
    "# the top terms per topic above, our LDA model seems to have generated\n",
    "# distributions that match what we might expect fairly well, though \n",
    "# since this is unsupervised, there may be some surprises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recommendations using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. (3pts) Generate Similarity Matrix\n",
    "\n",
    "# We'll use Content-Based Filtering to make recommendations based on a query product.\n",
    "# Each product will be represented by its LDA topic weights learned above (X_lda).\n",
    "# We'd like to recommend similar products in LDA space.\n",
    "# We'll use cosine_distance as our measure of similarity.\n",
    "\n",
    "# Import cosine_distances from sklearn.metrics.pairwise\n",
    "____\n",
    "\n",
    "# Use cosine_distance to generate similarity scores on our X_lda data\n",
    "# Store as distances\n",
    "# NOTE: we only need to pass X_lda in once as an argument,\n",
    "#   the function will calculate pairwise distance between all rows in that matrix\n",
    "____\n",
    "\n",
    "# print the shape of the distances matrix (should be 5000 x 5000)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. (4pts) Find Recommended Products\n",
    "\n",
    "# Let's test our proposed recommendation engine using the product at row 0 in df_jcp.\n",
    "#   The name of this product is \"Mixit™ Silver-Tone and Purple Enamel Earring and Pendant Necklace Set\"\n",
    "#   Our system will recommend products similiar to this product.\n",
    "\n",
    "# Print the names for the top 10 most similar products to this query.\n",
    "# Suggested way to do this is:\n",
    "#   get the cosine distances from row 0 of the distances matrix\n",
    "#   get the indices of this first row of distances sorted by value ascending using np.argsort()\n",
    "#   get the first 10 indexes from this sorted array of indices\n",
    "#   use those indices to index into df_jcp.name_title \n",
    "#   to get the full string, use .values\n",
    "#   print the resulting array\n",
    "\n",
    "# HINT: The first two products will likely be:\n",
    "#   'Mixit™ Silver-Tone and Purple Enamel Earring and Pendant Necklace Set',\n",
    "#   '1/7 CT. T.W. Diamond 10K White Gold Pendant'\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment Analysis Using Pipelines\n",
    "\n",
    "Here we will train a model to classify positive vs negative sentiment on a set of pet supply product reviews using sklearn Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. (2pts) Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product reviews\n",
    "#   of pet supply items on Amazon.\n",
    "# This data is taken from https://nijianmo.github.io/amazon/index.html\n",
    "#   Justifying recommendations using distantly-labeled reviews and fined-grained aspects\n",
    "#   Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "#   Empirical Methods in Natural Language Processing (EMNLP), 2019\n",
    "\n",
    "# Load product reviews from ../data/amazon-petsupply-reviews_subset.csv.zips\n",
    "# Use pandas read_csv function with the default parameters as in part 1.\n",
    "# Store the resulting dataframe as df_amzn.\n",
    "____\n",
    "\n",
    "# print a summary of df_amzn using .info()\n",
    "# there should be 10000 rows with 2 columns\n",
    "____\n",
    "\n",
    "# print the first row of the dataframe as an example\n",
    "# you should see the beginning of the review and the associated rating\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. (2pts) Transform Target\n",
    "\n",
    "# The ratings are originally in a 5 point scale\n",
    "# We'll turn this into a binary classification task to approximate positive vs negative sentiment\n",
    "\n",
    "# Print the proportions of values seen in the rating column\n",
    "#   using value_counts() with normalize=True\n",
    "____\n",
    "\n",
    "# Create a new binary target by setting\n",
    "#  rows where rating is 5 to True\n",
    "#  rows where rating is not 5 to False\n",
    "# Store in y\n",
    "____\n",
    "\n",
    "# adding an empty print statment to insert an empty line in the output\n",
    "print()\n",
    "\n",
    "# Print the proportions of values seen in y\n",
    "#   using value_counts() with normalize=True\n",
    "# True here means a rating of 5 (eg positive)\n",
    "# False means a rating less than 5 (eg negative)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. (1pts) Train-test split\n",
    "\n",
    "# Import train_test_split from sklearn\n",
    "____\n",
    "\n",
    "# Split df_amzn.review and y into a train and test set\n",
    "#   using train_test_split\n",
    "#   with test_size = .2 and stratifying by y\n",
    "# Store as reviews_train,reviews_test,y_train,y_test\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. (3pts) Create a Pipeline of TfIdf transformation and Classification\n",
    "\n",
    "# import Pipeline, GradientBoostingClassifier from sklearn\n",
    "____\n",
    "\n",
    "# Create a pipeline with two steps: \n",
    "#  TfIdfVectorizer with min_df=5 and max_df=.5 named 'tfidf'\n",
    "#  GradientBoostingClassifier with 200 trees named 'gbc'\n",
    "# Store as pipe_gbc\n",
    "____\n",
    "\n",
    "# Print out the pipeline\n",
    "# You should see both steps: tfidf and gbc\n",
    "print(pipe_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. (5pts) Perform Grid Search on pipe_gbc\n",
    "\n",
    "# import GridSearchCV from sklearn\n",
    "____\n",
    "\n",
    "# Create a parameter grid to test using:\n",
    "#   unigrams or unigrams + bigrams in the tfidf step\n",
    "#   max_depth of 2 or 10 in the gbc step\n",
    "# Store as param_grid\n",
    "____\n",
    "\n",
    "# Instantiate GridSearchCV to evaluate pipe_gbc on the values in param_grid\n",
    "#   use cv=2 and n_jobs=-1 to reduce run time\n",
    "# Fit on the training set\n",
    "# Store as gs_pipe_gbc\n",
    "____\n",
    "\n",
    "# Print the best parameter settings in gs_pipe_gbc found by grid search\n",
    "____\n",
    "\n",
    "# Print the best cv score found by grid search, with a precision of 2\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. (1 pts) Evaluate on the test set\n",
    "\n",
    "# Calculate the test set score using the fit gs_pipe_gbc \n",
    "#   to give confidence that we have not overfit\n",
    "#   while still improving over a random baseline classifier\n",
    "# Print the accuracy score on the test set with a precision of 2\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. (1 pts) Evaluate on example reviews\n",
    "\n",
    "# Generate predictions for these two sentences using the fit gs_pipe_gbc:\n",
    "#   'This is a great product.'\n",
    "#   'This product is not great.'\n",
    "# You should see True for the first (rating of 5) \n",
    "#   and False for the second (rating of less than 5)\n",
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f21",
   "language": "python",
   "name": "eods-f21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
